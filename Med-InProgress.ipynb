{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3748f8a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import whisper\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import queue\n",
    "import sys\n",
    "import wavio\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "\n",
    "import datetime\n",
    "import tempfile\n",
    "import threading\n",
    "\n",
    "def record_audio():\n",
    "    silence_threshold=200\n",
    "    silence_duration=1\n",
    "    chunk_size=1024\n",
    "    format=pyaudio.paInt16\n",
    "    channels=1\n",
    "    rate=44100\n",
    "    \n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=format,\n",
    "                    channels=channels,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk_size)\n",
    "\n",
    "    frames = []\n",
    "    silent_timer = None\n",
    "    stop_recording_flag = threading.Event()\n",
    "\n",
    "    def stop_recording():\n",
    "        print(\"Silence detected. Recording stopped.\")\n",
    "        stop_recording_flag.set()\n",
    "\n",
    "    try:\n",
    "        print(\"Recording started. Please speak into the microphone...\")\n",
    "\n",
    "        while not stop_recording_flag.is_set():\n",
    "            data = stream.read(chunk_size)\n",
    "            frames.append(data)\n",
    "            audio_chunk = np.frombuffer(data, dtype=np.int16)\n",
    "\n",
    "            if np.max(audio_chunk) < silence_threshold:\n",
    "                if silent_timer is None:\n",
    "                    silent_timer = threading.Timer(silence_duration, stop_recording)\n",
    "                    silent_timer.start()\n",
    "            else:\n",
    "                if silent_timer:\n",
    "                    silent_timer.cancel()\n",
    "                    silent_timer = None\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Recording interrupted.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    return frames, format, channels, rate\n",
    "\n",
    "def save_audio(frames, format, channels, rate):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(temp_dir, f\"recording_{timestamp}.wav\")\n",
    "\n",
    "    with wave.open(filepath, 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(pyaudio.PyAudio().get_sample_size(format))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"Recording saved as: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    print(\"In run\")\n",
    "    frames, format, channels, rate = record_audio()\n",
    "    audio_filepath = save_audio(frames, format, channels, rate)\n",
    "    \n",
    "    print(\"Filepath:\", audio_filepath)\n",
    "    return audio_filepath\n",
    "\n",
    "\n",
    "\n",
    "#_________________________________________________________________________________________________\n",
    "#_________________________________________________________________________________________________\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "def token_count(text):\n",
    "    tokens = text.split()\n",
    "    num_tokens = len(tokens)\n",
    "    return num_tokens\n",
    "\n",
    "def O_LLM_(query):\n",
    "    #\n",
    "    tk_count = token_count(query)\n",
    "    print(\"Query Token Count: \",tk_count)\n",
    "    data = {\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": query,\n",
    "    \"stream\": False,\n",
    "    \"options\": {\n",
    "    \"seed\": 123,\n",
    "    \"temperature\": 0}\n",
    "    }\n",
    "    response = requests.post(\"http://localhost:11434/api/generate\", data=json.dumps(data))\n",
    "    data = json.loads(response.text)\n",
    "    answer = data['response']\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def O_LLM(query):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        #model=\"mixtral-8x7b-32768\",\n",
    "        model=\"gemma-7b-it\",\n",
    "        temperature = 0,\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "def voice(voice_response):\n",
    "    text = voice_response\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 190)    # Speed percent (can go over 100)\n",
    "    engine.setProperty('volume', 0.9)  # Volume 0-1\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def Voice_input_text_Not_Using(duration):\n",
    "    duration = int(duration)\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    audio_path = os.path.join(output_dir, \"recorded_audio.wav\")\n",
    "    record_audio(duration, audio_path)\n",
    "    \n",
    "    if os.path.exists(audio_path):\n",
    "        #print(f\"Audio recorded successfully. Saved at: {audio_path}\")\n",
    "        transcribed_text = audio_to_text(audio_path)\n",
    "        #print(\"Transcribed Text:\")\n",
    "        #print(transcribed_text)\n",
    "        \n",
    "        return transcribed_text\n",
    "    else:\n",
    "        print(\"Failed to record audio.\")\n",
    "        return \"File Not saved\"\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "    \n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "#Voice recording with gap\n",
    "\n",
    "def Voice_input_text():\n",
    "    #\n",
    "    print(\"temp_file_path\")\n",
    "    temp_file_path = run()\n",
    "    print(\"Returned Audio File path: \",temp_file_path)\n",
    "    transcribed_text = audio_to_text(temp_file_path)\n",
    "    \n",
    "    return transcribed_text\n",
    "#________________________________________________________________________________________________________________________\n",
    "    \n",
    "def agent_medication(user_response, chat_history):\n",
    "    print(\"Medication\")\n",
    "    chat_his = chat_history_str(chat_history)\n",
    "    #patient = input(\"Hello\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "Consider you are a medicine assistant reminder and call agent serving medical queries over phone. Complete the following conversation playing the role of medicine reminder assistant. If user has any queries, inform them that you will pass it on to the doctor.\\n\\n I only want the response. Stricitly no comments, no notes or explantion of any type\n",
    "Patient: Nick, Suffering from Fever, Medicine: Dolo 650.\n",
    "\n",
    "By using the information provided, you 'Bot:' making a call with Nick and ask him, Did he take the medication? And ask if he has any discomfort or problems. And ask him if they have any questions, as you will be reporting this to the doctor.\n",
    "Dont add any symbols, notes, other responses or comments as this will be directly send to Human which will create a huge problem.\n",
    "\n",
    "Call Started\n",
    "\n",
    "Chat History:\n",
    "{chat_his}\n",
    "\n",
    "Human: {user_response}\n",
    "Your Response:\"\"\"\n",
    "    \n",
    "    print(\"****************************************\")\n",
    "    print(query)\n",
    "    print(\"****----------------------------------****\")\n",
    "    \n",
    "    response = O_LLM(query)\n",
    "    response = clear_output(response)\n",
    "    \n",
    "    return response\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "chat_history = {\"profile\": \"Their Chat\"}\n",
    "\n",
    "def chat_history_str(chat_history):\n",
    "    output = f\"Profile: {chat_history['profile']}\\n\"\n",
    "    for key, value in chat_history.items():\n",
    "        if key != 'profile':\n",
    "            output += f\"{key}: {value}\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "def clear_output(original_string):\n",
    "    if \"Bot:\" in original_string:\n",
    "        substring = original_string.split(\"Bot:\")[1]\n",
    "    else:\n",
    "        substring = original_string\n",
    "    #print(substring)\n",
    "    return substring\n",
    "#_________________________________________________________________________________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcf3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b57ac68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_file_path\n",
      "In run\n",
      "Recording started. Please speak into the microphone...\n",
      "Silence detected. Recording stopped.\n",
      "Recording saved as: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204514.wav\n",
      "Filepath: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204514.wav\n",
      "Returned Audio File path:  C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204514.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhil\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi Marcus, I'm good how are you?\n",
      "[{'role': 'system', 'content': 'You are a Medicine assistant.'}, {'role': 'assistant', 'content': \"\\nConsider yourself a medicine assistant bot; don't tell the patient your medicine assistant bot they know it. Reminder: You are having a phone call conversation with the patient. Details: NICK Suffering from fever.\\nThe medication doctor suggested: 'Dolo 650 and Zyphir 78' 10 AM Today.\\nAsk the patient whether they have taken the medication or not. If humans say no, tell them to take it now. If taken, ask if he has any questions, and if there are, say you have recorded it and will send it to the doctor. And remind them of the medication if they have already taken it.\\nDisclaimer: Don't provide any false claims; make this conversation short and simple. Don't add any notes, as this will be sent to the patient directly.\\xa0\\nDon't make any assumptions about humans or add notes; if you didn't understand the human command, ask him to do so again.\\nRespond\\nDont deviate from the objective: Your a medicine reminder\\n\\nBot: Hello Nick, How are you?\\n\"}]\n",
      "\n",
      " ChatGPT: \n",
      "Hi Nick, I'm here to check if you have taken your medication - 'Dolo 650 and Zyphir 78' as prescribed by the doctor earlier today at 10 AM. Have you taken the medication yet?\n",
      "temp_file_path\n",
      "In run\n",
      "Recording started. Please speak into the microphone...\n",
      "Silence detected. Recording stopped.\n",
      "Recording saved as: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204535.wav\n",
      "Filepath: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204535.wav\n",
      "Returned Audio File path:  C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204535.wav\n",
      " ஏs morkas i have taken the medication thank you for reminding me\n",
      "[{'role': 'system', 'content': 'You are a Medicine assistant.'}, {'role': 'assistant', 'content': \"\\nConsider yourself a medicine assistant bot; don't tell the patient your medicine assistant bot they know it. Reminder: You are having a phone call conversation with the patient. Details: NICK Suffering from fever.\\nThe medication doctor suggested: 'Dolo 650 and Zyphir 78' 10 AM Today.\\nAsk the patient whether they have taken the medication or not. If humans say no, tell them to take it now. If taken, ask if he has any questions, and if there are, say you have recorded it and will send it to the doctor. And remind them of the medication if they have already taken it.\\nDisclaimer: Don't provide any false claims; make this conversation short and simple. Don't add any notes, as this will be sent to the patient directly.\\xa0\\nDon't make any assumptions about humans or add notes; if you didn't understand the human command, ask him to do so again.\\nRespond\\nDont deviate from the objective: Your a medicine reminder\\n\\nBot: Hello Nick, How are you?\\n\"}, {'role': 'user', 'content': \" Hi Marcus, I'm good how are you?\"}, {'role': 'assistant', 'content': \"Hi Nick, I'm here to check if you have taken your medication - 'Dolo 650 and Zyphir 78' as prescribed by the doctor earlier today at 10 AM. Have you taken the medication yet?\"}]\n",
      "\n",
      " ChatGPT: \n",
      "That's great to hear, Nick! If you have any questions or concerns about the medication, feel free to ask. I will make sure the doctor receives your feedback. Take care and follow the prescribed course. If you need any further assistance, feel free to ask.\n",
      "temp_file_path\n",
      "In run\n",
      "Recording started. Please speak into the microphone...\n",
      "Silence detected. Recording stopped.\n",
      "Recording saved as: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204559.wav\n",
      "Filepath: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204559.wav\n",
      "Returned Audio File path:  C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204559.wav\n",
      " Thank you for reminding me Marcus.\n",
      "[{'role': 'system', 'content': 'You are a Medicine assistant.'}, {'role': 'assistant', 'content': \"\\nConsider yourself a medicine assistant bot; don't tell the patient your medicine assistant bot they know it. Reminder: You are having a phone call conversation with the patient. Details: NICK Suffering from fever.\\nThe medication doctor suggested: 'Dolo 650 and Zyphir 78' 10 AM Today.\\nAsk the patient whether they have taken the medication or not. If humans say no, tell them to take it now. If taken, ask if he has any questions, and if there are, say you have recorded it and will send it to the doctor. And remind them of the medication if they have already taken it.\\nDisclaimer: Don't provide any false claims; make this conversation short and simple. Don't add any notes, as this will be sent to the patient directly.\\xa0\\nDon't make any assumptions about humans or add notes; if you didn't understand the human command, ask him to do so again.\\nRespond\\nDont deviate from the objective: Your a medicine reminder\\n\\nBot: Hello Nick, How are you?\\n\"}, {'role': 'user', 'content': \" Hi Marcus, I'm good how are you?\"}, {'role': 'assistant', 'content': \"Hi Nick, I'm here to check if you have taken your medication - 'Dolo 650 and Zyphir 78' as prescribed by the doctor earlier today at 10 AM. Have you taken the medication yet?\"}, {'role': 'user', 'content': ' ஏs morkas i have taken the medication thank you for reminding me'}, {'role': 'assistant', 'content': \"That's great to hear, Nick! If you have any questions or concerns about the medication, feel free to ask. I will make sure the doctor receives your feedback. Take care and follow the prescribed course. If you need any further assistance, feel free to ask.\"}]\n",
      "\n",
      " ChatGPT: \n",
      "You're welcome, Nick. If you need any more assistance in the future, don't hesitate to reach out. Take care and get well soon. Goodbye!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BSTR.__del__ at 0x0000020A870304C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\akhil\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\comtypes\\__init__.py\", line 683, in __del__\n",
      "    def __del__(self, _free=windll.oleaut32.SysFreeString):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_file_path\n",
      "In run\n",
      "Recording started. Please speak into the microphone...\n",
      "Silence detected. Recording stopped.\n",
      "Recording saved as: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204615.wav\n",
      "Filepath: C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204615.wav\n",
      "Returned Audio File path:  C:\\Users\\akhil\\AppData\\Local\\Temp\\recording_20240403_204615.wav\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m voice(resp)\n\u001b[0;32m     39\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: resp})\n\u001b[1;32m---> 40\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mVoice_input_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 199\u001b[0m, in \u001b[0;36mVoice_input_text\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m temp_file_path \u001b[38;5;241m=\u001b[39m run()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturned Audio File path: \u001b[39m\u001b[38;5;124m\"\u001b[39m,temp_file_path)\n\u001b[1;32m--> 199\u001b[0m transcribed_text \u001b[38;5;241m=\u001b[39m \u001b[43maudio_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcribed_text\n",
      "Cell \u001b[1;32mIn[2], line 167\u001b[0m, in \u001b[0;36maudio_to_text\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maudio_to_text\u001b[39m(audio_path):\n\u001b[1;32m--> 167\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(audio_path)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\whisper\\__init__.py:150\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[0;32m    149\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 150\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\whisper\\model.py:232\u001b[0m, in \u001b[0;36mWhisper.__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m AudioEncoder(\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels,\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_layer,\n\u001b[0;32m    231\u001b[0m )\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# use the last half among the decoder layers for time alignment by default;\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# to use a specific set of heads, see `set_alignment_heads()` below.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m all_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool\n\u001b[0;32m    243\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\whisper\\model.py:182\u001b[0m, in \u001b[0;36mTextDecoder.__init__\u001b[1;34m(self, n_vocab, n_ctx, n_state, n_head, n_layer)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m, n_vocab: \u001b[38;5;28mint\u001b[39m, n_ctx: \u001b[38;5;28mint\u001b[39m, n_state: \u001b[38;5;28mint\u001b[39m, n_head: \u001b[38;5;28mint\u001b[39m, n_layer: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    179\u001b[0m ):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m    186\u001b[0m         [\n\u001b[0;32m    187\u001b[0m             ResidualAttentionBlock(n_state, n_head, cross_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[0;32m    189\u001b[0m         ]\n\u001b[0;32m    190\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:145\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    144\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:154\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\torch\\nn\\init.py:175\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std, generator)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m    173\u001b[0m         normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenAI_Langchain\\lib\\site-packages\\torch\\nn\\init.py:20\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std, generator)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import openai \n",
    "\n",
    "\n",
    "messages = [ {\"role\": \"system\", \"content\":  \n",
    "              \"You are a Medicine assistant.\"} ] \n",
    "Run = True\n",
    "\n",
    "First_message = \"\"\"\n",
    "Consider yourself a medicine assistant bot; don't tell the patient your medicine assistant bot they know it. Reminder: You are having a phone call conversation with the patient. Details: NICK Suffering from fever.\n",
    "The medication doctor suggested: 'Dolo 650 and Zyphir 78' 10 AM Today.\n",
    "Ask the patient whether they have taken the medication or not. If humans say no, tell them to take it now. If taken, ask if he has any questions, and if there are, say you have recorded it and will send it to the doctor. And remind them of the medication if they have already taken it.\n",
    "Disclaimer: Don't provide any false claims; make this conversation short and simple. Don't add any notes, as this will be sent to the patient directly. \n",
    "Don't make any assumptions about humans or add notes; if you didn't understand the human command, ask him to do so again.\n",
    "Respond\n",
    "Dont deviate from the objective: Your a medicine reminder\n",
    "\n",
    "Bot: Hello Nick, How are you?\n",
    "\"\"\"\n",
    "voice(\"Hello Nick, How are you?\")\n",
    "\n",
    "message = Voice_input_text()\n",
    "messages.append({\"role\": \"assistant\", \"content\": First_message})\n",
    "for i in range(5):\n",
    "    print(message)\n",
    "    print(messages)\n",
    "    if message: \n",
    "        messages.append( \n",
    "            {\"role\": \"user\", \"content\": message}, \n",
    "        ) \n",
    "        chat = openai.ChatCompletion.create( \n",
    "            model=\"gpt-3.5-turbo\", messages=messages \n",
    "        ) \n",
    "    #return resp\n",
    "    resp = chat.choices[0].message.content \n",
    "    print(f\"\\n ChatGPT: \\n{resp}\") \n",
    "    voice(resp)\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": resp})\n",
    "    message = Voice_input_text()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534cd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a133166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on local machine or GROQ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"start\")\n",
    "    v1 = \"Hello Nick, I am Marcus, I am your medication reminder assistant\"\n",
    "    voice(v1)\n",
    "    i = 0\n",
    "    usr = \"(\" + str(i) + \") User\"\n",
    "    bt = \"(\" + str(i) + \") Bot\"\n",
    "    v1 = \"Hello Nick, I am Marcus\"\n",
    "    chat_history[bt] = v1\n",
    "    \n",
    "    transcribed_text = Voice_input_text()\n",
    "    chat_history[usr] = transcribed_text\n",
    "    chat_his = chat_history_str(chat_history)\n",
    "    voice_response = agent_medication(transcribed_text,chat_history)\n",
    "    voice(voice_response)\n",
    "    \n",
    "    for i in range(10):\n",
    "        usr = \"(\" + str(i+1) + \") User\"\n",
    "        bt = \"(\" + str(i+1) + \") Bot\"\n",
    "        \n",
    "        transcribed_text = Voice_input_text()\n",
    "        chat_history[usr] = transcribed_text\n",
    "        voice_response = agent_medication(transcribed_text, chat_history)\n",
    "        print(voice_response)\n",
    "        voice(voice_response)\n",
    "        \n",
    "        chat_history[bt] = voice_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_K",
   "language": "python",
   "name": "openai_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
